{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3133,
     "status": "ok",
     "timestamp": 1670274700839,
     "user": {
      "displayName": "Ember Richardson",
      "userId": "05921004582757010193"
     },
     "user_tz": 300
    },
    "id": "ie76vEKeBDAv",
    "outputId": "fc494b1a-0a09-4678-c862-cbda099ec881"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import typing\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from numpy.lib import recfunctions as rfn\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab config\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#os.chdir('/content/drive/My Drive/cs470')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "The dataset was downloaded from Kaggle. It contains 32 Columns of 1,000,000 entries. Since it is 216.59 MB, it may take time to load.\n",
    "\n",
    "_in this project, \"label\" will refer to column categories, and \"data_actual\" will refer to the actual value of the data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Base.csv\") # load csv from drive\n",
    "classification = data.pop(\"fraud_bool\").tolist() # remove classification label from dataset\n",
    "labels = data.keys().tolist() # get column labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot vectorization\n",
    "\n",
    "Machine learning algorithms have a hard time understanding strings. To confront this, we replace a column of multiple unique string values with multiple columns for each unique category. These columns contain a boolean to indicate which category it was. This is alternatively known as a _One-hot_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hotify labels\n",
    "labels_categorical = [\"payment_type\", \"employment_status\", \"housing_status\", \"source\", \"device_os\"] # list that contains columns to be binarized\n",
    "labels_vectorized = [] # list that contains all newly created binarized columns\n",
    "for label in labels_categorical:\n",
    "    label_index = data.columns.get_loc(label)\n",
    "    column_binarized = pd.get_dummies(data[label])\n",
    "    for label_binarized in column_binarized:\n",
    "        # new label joins the category with the original column name\n",
    "        label_binarized_new = label + \"_\" + label_binarized\n",
    "        data.insert(\n",
    "            label_index,\n",
    "            label_binarized_new,\n",
    "            column_binarized[label_binarized])\n",
    "        labels_vectorized.append(label_binarized_new)\n",
    "    del data[label]\n",
    "# prove that removal occurred and that new columns were added\n",
    "assert(\"payment_type\" not in data.columns and \"device_os\" not in data.columns and \"device_os_windows\" in data.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum-Maximum Normalization\n",
    "\n",
    "The backpropagation algorithm will eventually attempt to normalize the range of values within a column to become from 0 to 1. We can save time by preprocessing the data beforehand, shaving seconds if not minutes off of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum-maximum normalization\n",
    "scaler = MinMaxScaler() # scales data between 0 and 1\n",
    "for label in labels:\n",
    "    if label not in labels_categorical: # check only the ones that were not categorical (this implies columns of numbers, not strings)\n",
    "        data[label] = scaler.fit_transform(data[[label]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Partitioning\n",
    "\n",
    "Finally, the data will be partitioned into both training and evaluation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data partitioning\n",
    "training_ratio = 0.85 # this percent will be used for training\n",
    "training_index = math.floor(len(data)*training_ratio)\n",
    "data_training = (data[:training_index]) # training data\n",
    "labels_training = (classification[:training_index]) # training data's actual value\n",
    "data_evaluation = (data[training_index:]) # evaluation data\n",
    "labels_evaluation  = (classification[training_index:]) # evaluation data's actual value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Guq81mAAGG_L"
   },
   "source": [
    "# Model Design\n",
    "\n",
    "We will be using a simple Multilayer Neural Network for this project. This is because our data is simply just numbers, which have no spatial significance (like an image), so we will not be using a Convolutional Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "NHTyDJ31HqHb"
   },
   "outputs": [],
   "source": [
    "# model design\n",
    "model: keras.Model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')]) # output layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(),tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.TruePositives()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      2\u001b[0m     x\u001b[39m=\u001b[39;49mdata_training,\n\u001b[1;32m      3\u001b[0m     y\u001b[39m=\u001b[39;49mlabels_training,\n\u001b[1;32m      4\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/keras/engine/data_adapter.py:1083\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1080\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m   1081\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1084\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1085\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(_type_name(x), _type_name(y))\n\u001b[1;32m   1086\u001b[0m     )\n\u001b[1;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1088\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[1;32m   1092\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=data_training,\n",
    "    y=labels_training,\n",
    "    epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688/4688 [==============================] - 28s 6ms/step - loss: 0.0413 - binary_accuracy: 0.9903 - precision: 0.5000 - false_positives: 3.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.evaluate(evaluation_dataset, evaluation_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 16:07:14.649445: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 290ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00150064]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([evaluation_dataset[5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
